# Context Engineering Prefix (CEP) Implementation Status

## Goal
Train SLM with CEP to **beat LLM baseline performance**:
- **LLM Baseline**: 60% F1 (structured), 74% F1 (toolcall)
- **Existing Fledgling SLM**: 32% F1 (structured), 0% F1 (toolcall)
- **Target**: Beat LLM baseline (>60% structured, >74% toolcall)

## ‚úÖ What We Built

### 1. CEP Framework (`slm_swap/cep_config.py`)
- **Universal CEP**: Works for BOTH JSON structured outputs AND XML tool calls
- **Variants**: Universal, Compact, Structured-specific, Toolcall-specific
- **Addresses specific error patterns**:
  - ‚ùå Markdown wrappers (````json`)
  - ‚ùå Self-closing XML tags
  - ‚ùå Parameter name mismatches
  - ‚ùå Wrong `arguments=` attribute

**CEP Content**:
```
<|formatting_rules|>
CRITICAL OUTPUT RULES - FOLLOW EXACTLY:

1. JSON Outputs:
   - Output ONLY raw JSON object
   - NO markdown blocks (no ```json or ```)
   - Match parameter names EXACTLY as in signature

2. XML Tool Calls:
   - Format: <tool_call name="FUNC_NAME">{"arg": "value"}</tool_call>
   - MUST have opening tag: <tool_call name="...">
   - MUST have closing tag: </tool_call>
   - NO self-closing tags

3. Common Errors to AVOID:
   [Specific error examples with ‚ùå markers]

VERIFY: Does output match format rules? Check before responding.
<|end_formatting_rules|>
```

### 2. Training Scripts (Multiple Approaches Tested)
- `train_llama_cep.py` - Unsloth + SFTTrainer
- `train_llama_cep_fixed.py` - Unsloth + Standard Trainer
- `train_llama_cep_v2.py` - Unsloth + Custom Data Collator
- `train_llama_cep_pure.py` - Pure Transformers + PEFT (no Unsloth)
- `train_phi_cep*.py` - Corresponding Phi-4 versions

All scripts:
- Apply CEP to system prompts during training
- Use Hermes function calling dataset
- Implement LoRA fine-tuning
- Include proper tokenization and formatting

### 3. Workflow Tracer (`slm_swap/workflow_tracer.py`)
**Fully automatic continuous learning system**:
- Captures LLM workflow calls automatically
- Converts to Hermes-format training data
- Detects when enough data collected (100+ examples)
- Triggers automatic retraining notifications
- Tracks metrics: success rate, cost, execution time

**Usage**:
```python
from workflow_tracer import WorkflowTracer

tracer = WorkflowTracer()
tracer.capture_call(
    agent_type="orchestrator",
    agent_name="main",
    model_used="gpt-4",
    system_prompt="...",
    user_query="...",
    assistant_response="...",
    success=True
)

# Automatic dataset generation when threshold reached
dataset_path = tracer.generate_training_dataset()
```

## ‚ùå Critical Blockers

### Blocker 1: Unsloth Compatibility Issue
**Error**: `AttributeError: 'int' object has no attribute 'mean'`
- **Location**: `_unsloth_training_step` line 71
- **Versions**: Unsloth 2025.11.2 + Transformers 4.57.1
- **Impact**: ALL Unsloth-based training crashes during `trainer.train()`
- **Affects**: SFTTrainer, Standard Trainer, Custom Collators - all fail
- **Tried**: 10+ different configurations, all hit same error

### Blocker 2: Gated Model Access
**Error**: `403 Client Error: Forbidden` for `meta-llama/Llama-3.1-8B-Instruct`
- **Issue**: Pure Transformers (without Unsloth) can't load gated models without HF auth
- **Note**: Unsloth CAN load the model but crashes during training

### Blocker 3: GPU Driver Instability
- **Error**: `CUDA error: unknown error` after step 3/21 followed by `nvidia-smi` ‚Üí `Failed to initialize NVML`
- **dmesg**: Multiple `Xid 79` entries (`GPU has fallen off the bus`)
- **Impact**: All GPUs offline until the host is rebooted or the NVIDIA driver is reloaded (requires root)
- **Mitigation**:
  1. Reboot or reload the NVIDIA driver so `nvidia-smi` works again.
  2. Relaunch training via `./slm_swap/resume_llama_cep.sh` (adds health checks + consistent launch options).
  3. Monitor `slm_swap/logs/train_llama_cep_pure.log` for progress.

## üìä Current State

### Files Created
```
slm_swap/
‚îú‚îÄ‚îÄ cep_config.py                 # CEP framework ‚úÖ
‚îú‚îÄ‚îÄ workflow_tracer.py            # Automatic learning ‚úÖ
‚îú‚îÄ‚îÄ train_llama_cep.py           # Unsloth + SFTTrainer ‚ùå
‚îú‚îÄ‚îÄ train_llama_cep_fixed.py     # Unsloth + Trainer ‚ùå
‚îú‚îÄ‚îÄ train_llama_cep_v2.py        # Unsloth + Custom Collator ‚ùå
‚îú‚îÄ‚îÄ train_llama_cep_pure.py      # Pure PEFT ‚ùå (403 error)
‚îú‚îÄ‚îÄ train_phi_cep.py             # Phi versions ‚ùå
‚îú‚îÄ‚îÄ train_phi_cep_fixed.py
‚îú‚îÄ‚îÄ train_phi_cep_v2.py
‚îú‚îÄ‚îÄ resume_llama_cep.sh          # Relaunch helper + GPU health checks ‚úÖ
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ unsloth_blocker.txt      # Documented blocker
```

### Training Attempts
- **Total attempts**: 15+
- **Successful completions**: 0
- **Reason**: Unsloth training crash OR gated model access

## üîß Solutions to Unblock

### Option 1: HuggingFace Authentication
```bash
huggingface-cli login
# Then retry train_llama_cep_pure.py (Pure Transformers + PEFT)
```
- ‚úÖ Bypasses Unsloth crash
- ‚úÖ Can load gated models
- ‚ùå Slower training (no Unsloth optimization)

### Option 2: Downgrade Unsloth
```bash
pip install unsloth==2024.X  # Find compatible version
```
- ‚úÖ May fix training crash
- ‚ùå Unknown which version works
- ‚ùå May break other dependencies

### Option 3: Use Non-Gated Model
Switch to `mistralai/Mistral-7B-Instruct-v0.3` or similar:
- ‚úÖ No auth required
- ‚úÖ Can use Unsloth OR pure Transformers
- ‚ùå Different model family

### Option 4: Use Existing Fledgling + CEP Inference
Apply CEP at inference time to existing trained model:
- ‚úÖ No training needed
- ‚úÖ Tests CEP effectiveness immediately
- ‚ùå Not "baked in" to model weights

## üìù Next Steps (Once Unblocked)

1. **Recover GPU stack**
   - Reboot or reload NVIDIA driver until `nvidia-smi` works
   - Use `./slm_swap/resume_llama_cep.sh` to restart CEP training safely
2. **Complete Training**
   - Train Llama-3.1-8B with universal CEP
   - Train Phi-4 with compact CEP
   - Target: Beat LLM baseline (>60% F1 structured, >74% toolcall)

3. **Evaluation**
   - Run both Berkeley tracks
   - Compare CEP model vs LLM baseline
   - Measure improvement from CEP

4. **Deploy Workflow Tracer**
   - Integrate into production workflows
   - Automatic data collection
   - Continuous improvement loop

5. **Production Integration**
   - Orchestrator: Llama-3.1-8B + CEP
   - Sub-agents: Phi-4 + CEP
   - Automatic workflow capture ‚Üí retrain ‚Üí swap

## üí° Key Insights

1. **CEP Design**: Universal prefix that works for BOTH output formats is cleaner than separate approaches

2. **Error Patterns**: The Fledgling SLM failures (32% structured, 0% toolcall) match exactly the errors CEP addresses:
   - Markdown wrappers
   - Wrong XML format
   - Parameter mismatches

3. **Automatic Learning**: Workflow tracer enables true continuous improvement - capture LLM calls, retrain SLM, gradual swap

4. **FTP ‚Üí CEP**: Paper's Fine-Tuning Prefix concept generalizes well to context engineering for multiple output types

## üéØ Success Criteria

- ‚úÖ CEP framework implemented and tested
- ‚úÖ Workflow tracer fully functional
- ‚úÖ Training scripts ready
- ‚ùå Training blocked by technical issues
- ‚è≥ Pending: Beat LLM baseline performance

## Status: READY TO TRAIN (Once Blockers Resolved)

All infrastructure is in place. Need to resolve either:
- Unsloth compatibility issue, OR
- HuggingFace authentication for pure Transformers approach

Then execute training and validation to beat LLM baseline.
